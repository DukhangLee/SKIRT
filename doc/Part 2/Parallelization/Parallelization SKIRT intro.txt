/**

\page ParallelizationIntro Introduction to parallelization in SKIRT

The basic unit of parallelization in <tt>SKIRT</tt> is a chunk. A chunk is a certain number of photon packages of the same wavelength. 
Such a chunk is always simulated by a single thread. The total number of chunks is determined by the number of wavelengths in the 
simulation and the number of photon packages desired per wavelength. The parallelization of the oligochromatic and panchromatic simulations is 
designed in the same way: the \f$N_C \times N_{\lambda}\f$ chunks are distributed over the different threads, for different wavelengths at a time. 
The first chunks of each wavelength are handed out first. Whenever a thread has finished its work, it picks out a chunk of the wavelength that 
is next in line. 

The essence of a multithreaded parallelization is that the code executed by the different processors uses the same memory locations. 
The threads share the entire process state, with all variables and functions. This inevitably leads to so-called race conditions, 
where different threads want to read or write the same memory location at the same time. When one process acts on the value of a certain variable, 
during which another thread changes that same variable, inconsistencies occur. There are mechanisms that prevent this incorrect behaviour, 
such as locking. Locking basically means that all other threads are prevented from reading a certain variable 
until a certain thread has finished writing it. If used extensively however, with a large number of threads, 
this can ultimately lead to decreased performance.

With an MPI parallelization, the execution of parallel code is performed by multiple, independent processes, 
each with their own memory addresses and process state. This avoids the performance issues from which the multithreading suffers. 
On the other hand, this kind of parallelization requires the implementation of explicit calls to the MPI library at any point 
where communication is needed between processes: a process cannot just read the memory of an other process. 
If implemented efficiently with the minimal amount of communication, an MPI parallelized code scales much better with a 
high number of processors. This makes these codes perfectly suited for running on (distributed memory) multi-processor systems (or supercomputers). 

As each MPI process is basically a copy of the entire program, a program parallelized with \f$n\f$ parallel processes 
takes in \f$n\f$ times as much memory as the same program parallelized with n parallel threads. 
That could be a reason for choosing a multithreaded approach when the model size is too large (many wavelengths and/or a fine dust grid). 
When we talk about running a program with MPI, we of course assume the model fits on the system. 

We have implemented an MPI parallelization in SKIRT that follows the same design as the thread parallelization. 
That means: each process simulates all wavelengths, but each process simulates only a portion of the chunks. 
That will be the design presented in this text. In a later stage, if performance enhancements seem necessary, 
we can also choose to give each process only a limited amount of wavelengths. This procedure will minimize the communication overhead. 
Sometimes, wavelengths will however also need to be split over multiple processes if the number of processors exceeds the number of wavelengths.

*/
